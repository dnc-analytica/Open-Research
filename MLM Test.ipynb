{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65916d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c330e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b6f591c4d5f61f22\n",
      "Reusing dataset csv (C:\\Users\\paperspace\\.cache\\huggingface\\datasets\\csv\\default-b6f591c4d5f61f22\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"csv\", data_files=\"title_conference.csv\", split=\"train\", delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6d5aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=ds.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a9d155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    global tokenized_datasets\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "    def tokenize_function(examples):\n",
    "        #concat_str = [t + \" \" + s for t, s in zip(examples[\"title\"], examples[\"selftext\"])]\n",
    "        return tokenizer(examples[\"Title\"], padding='max_length', truncation=True, max_length = 64)\n",
    "    tokenized_datasets = ds['train'].map(tokenize_function, batched=True, num_proc = 4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3348c57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Title', 'Conference', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 2256\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09586ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets.cleanup_cache_files()\n",
    "#ds.cleanup_cache_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOT' > ds_config_zero3.json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": true\n",
    "        },\n",
    "        \"overlap_comm\": true,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e9,\n",
    "        \"reduce_bucket_size\": \"auto\",\n",
    "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
    "        \"stage3_param_persistence_threshold\": \"auto\",\n",
    "        \"stage3_max_live_parameters\": 1e9,\n",
    "        \"stage3_max_reuse_distance\": 1e9,\n",
    "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d7939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSpeed requires a distributed environment even when only one process is used.\n",
    "# This emulates a launcher in the notebook\n",
    "import os\n",
    "\n",
    "#os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "#os.environ[\"RANK\"] = \"0\"\n",
    "#os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "#os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "#os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\" # for windows OS\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"Distil_Model\",\n",
    "    per_device_train_batch_size = 32,\n",
    "    num_train_epochs = 200,\n",
    "    #evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"no\",\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    #deepspeed=\"ds_config_zero3.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1590a0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ce865e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: Title, Conference. If Title, Conference are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\paperspace\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2256\n",
      "  Num Epochs = 200\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14200' max='14200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14200/14200 59:56, Epoch 200/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.181600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.868800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.630400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.216300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.954000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.567700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.513200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.502500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.459800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14200, training_loss=0.7947192957703496, metrics={'train_runtime': 3598.6491, 'train_samples_per_second': 125.38, 'train_steps_per_second': 3.946, 'total_flos': 7491873746534400.0, 'train_loss': 0.7947192957703496, 'epoch': 200.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "339acd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to Distil_Model\n",
      "Configuration saved in Distil_Model\\config.json\n",
      "Model weights saved in Distil_Model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44d9c00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Distil_Model\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"Distil_Model\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file Distil_Model\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at Distil_Model were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at Distil_Model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"Distil_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae68fb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae5321e5ddc41cb96bccb8e67360146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2256 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ds_with_bert_embeddings_v2 = ds[\"train\"].map(lambda example: {'bert_embeddings': torch.mean(model(**tokenizer(example[\"Title\"], return_tensors=\"pt\")).last_hidden_state, dim = 1).squeeze().numpy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e6dec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e62abc430844bbba9aa710aa084fa1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_with_bert_embeddings_v2.add_faiss_index(column='bert_embeddings')\n",
    "def predict(query):\n",
    "    with torch.no_grad():\n",
    "        question_embedding = torch.mean(model(**tokenizer(query, return_tensors=\"pt\")).last_hidden_state, dim = 1).squeeze().numpy()\n",
    "    \n",
    "    scores, retrieved_examples = ds_with_bert_embeddings_v2.get_nearest_examples('bert_embeddings', question_embedding, k=3)\n",
    "    return retrieved_examples[\"Conference\"][0] + \"|\" + retrieved_examples[\"Title\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4ecc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\paperspace\\.cache\\huggingface\\datasets\\csv\\default-b6f591c4d5f61f22\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-18787776b1735576.arrow\n",
      "Loading cached processed dataset at C:\\Users\\paperspace\\.cache\\huggingface\\datasets\\csv\\default-b6f591c4d5f61f22\\0.0.0\\652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a\\cache-fa0044f1d62893dd.arrow\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "please remove all the indexes using `dataset.drop_index` before saving a dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m ds_with_bert_embeddings_v2\u001b[38;5;241m.\u001b[39msave_faiss_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved_Datasets\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbert_embeddings.faiss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ds_with_bert_embeddings_v2\u001b[38;5;241m.\u001b[39mdrop_index\n\u001b[1;32m----> 5\u001b[0m \u001b[43mds_with_bert_embeddings_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSaved_Datasets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:1086\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[1;34m(self, dataset_path, fs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;124;03mSaves a dataset to a dataset directory, or in a filesystem using either :class:`~filesystems.S3FileSystem` or\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;124;03many implementation of ``fsspec.spec.AbstractFileSystem``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_indexes():\n\u001b[1;32m-> 1086\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease remove all the indexes using `dataset.drop_index` before saving a dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1088\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_indices() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_filesystem(fs):\n",
      "\u001b[1;31mValueError\u001b[0m: please remove all the indexes using `dataset.drop_index` before saving a dataset"
     ]
    }
   ],
   "source": [
    "ds.save_to_disk(\"Saved_Datasets\")\n",
    "\n",
    "ds_with_bert_embeddings_v2.save_faiss_index('bert_embeddings', 'Saved_Datasets\\\\bert_embeddings.faiss')\n",
    "ds_with_bert_embeddings_v2.drop_index\n",
    "ds_with_bert_embeddings_v2.save_to_disk(\"Saved_Datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1f3222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb2f22306354c9a8a8b2d85fe48e03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    ds_with_test_predictions = ds[\"test\"].map(lambda example: {'Predictions': predict(example[\"Title\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45fe109e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eed21cc6c2e492f97e609618bbd24d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "39450"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_with_test_predictions.to_csv(\"distil_predictions_conference_v5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62c961d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
